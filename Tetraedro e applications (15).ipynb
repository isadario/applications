{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea683cd",
   "metadata": {},
   "source": [
    "# 1. Loading and reading of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded the dataset from the UCI repository archive \n",
    "#(https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease)\n",
    "\n",
    "#decompressed the RAR file \n",
    "#turned the arff file into csv (using Python converter)\n",
    "\n",
    "# now loading full csv into the notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16976d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Chronic_Kidney_Disease/chronic_kidney_disease_full.csv', header = 0, on_bad_lines='skip')\n",
    "\n",
    "#there are only 25 features so no need to load only a subset of the features and no need to use chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8c091",
   "metadata": {},
   "source": [
    "# 2. Exploring, handling missing data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataset is about chronic kidney disease\n",
    "\n",
    "#we are trying to model whether or not somebody has a chronic kidney disease (\"class\")\n",
    "\n",
    "#starting from a dataset of 397 individuals \n",
    "#with 25 features recorded \n",
    "#out of which 11 are numerical and 14 nominal \n",
    "\n",
    "\n",
    "#The variables range from \n",
    "\n",
    "#1.Age(numerical)\n",
    "#age in years\n",
    "#2.Blood Pressure(numerical)\n",
    "#bp in mm/Hg\n",
    "#3.Specific Gravity(nominal)\n",
    "#sg - (1.005,1.010,1.015,1.020,1.025)\n",
    "#4.Albumin(nominal)\n",
    "#al - (0,1,2,3,4,5)\n",
    "#5.Sugar(nominal)\n",
    "#su - (0,1,2,3,4,5)\n",
    "#6.Red Blood Cells(nominal)\n",
    "#rbc - (normal,abnormal)\n",
    "#7.Pus Cell (nominal)\n",
    "#pc - (normal,abnormal)\n",
    "#8.Pus Cell clumps(nominal)\n",
    "#pcc - (present,notpresent)\n",
    "#9.Bacteria(nominal)\n",
    "#ba - (present,notpresent)\n",
    "#10.Blood Glucose Random(numerical)\n",
    "#bgr in mgs/dl\n",
    "#11.Blood Urea(numerical)\n",
    "#bu in mgs/dl\n",
    "#12.Serum Creatinine(numerical)\n",
    "#sc in mgs/dl\n",
    "#13.Sodium(numerical)\n",
    "#sod in mEq/L\n",
    "#14.Potassium(numerical)\n",
    "#pot in mEq/L\n",
    "#15.Hemoglobin(numerical)\n",
    "#hemo in gms\n",
    "#16.Packed Cell Volume(numerical)\n",
    "#17.White Blood Cell Count(numerical)\n",
    "#wc in cells/cumm\n",
    "#18.Red Blood Cell Count(numerical)\n",
    "#rc in millions/cmm\n",
    "#19.Hypertension(nominal)\n",
    "#htn - (yes,no)\n",
    "#20.Diabetes Mellitus(nominal)\n",
    "#dm - (yes,no)\n",
    "#21.Coronary Artery Disease(nominal)\n",
    "#cad - (yes,no)\n",
    "#22.Appetite(nominal)\n",
    "#appet - (good,poor)\n",
    "#23.Pedal Edema(nominal)\n",
    "#pe - (yes,no)\n",
    "#24.Anemia(nominal)\n",
    "#ane - (yes,no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea8892",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning obvious errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the quotes from the label of the columns                              #DATAFRAME\n",
    "\n",
    "print(\"Before is\", data.columns.tolist())\n",
    "\n",
    "data.columns = [col[1:-1] for col in data.columns]\n",
    "\n",
    "print(\"After is\", data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0488e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by looking above there is a problem with class because it says unique 3                 #CLASS\n",
    "#but the classification is binary so should be 2 \n",
    "print(\"Class categories are\", len(data['class'].unique()), \"should be 2 instead\")\n",
    "print(\"Actual categories for the class variable are\", data['class'].unique())\n",
    "\n",
    "#correct the error\n",
    "#data.loc[(data['class'] =='ckd\\t')]\n",
    "data['class'].replace({\"ckd\\t\": \"ckd\"}, inplace=True)\n",
    "print(\"Now only\", len(data['class'].unique()),\"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#by looking above most of the categorical variables are now fixed\n",
    "#(ie. have 2 values) \n",
    "#but for example I see cad (which stands for Coronary Artery Disease \n",
    "#so yes/no) with 3 values \n",
    "print(\"cad categories are\", len(data['cad'].unique()), \"should be 2 instead\")\n",
    "print(\"currently the categories for this feature are\", data['cad'].unique())\n",
    "\n",
    "#clearly '\\tno' is an input error so we correct it for the 2 rows when there is \n",
    "#len(data.loc[(data['cad'] =='\\tno')])\n",
    "\n",
    "data['cad'].replace({\"\\tno\": \"no\"}, inplace=True)\n",
    "print(\"Now only\", len(data['cad'].unique())-1,\"classes\") #-1 for the nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I spot a similar issue with with anemia, pe, appet             #FEATURES\n",
    "#so there is probably an issue with categorical variables \n",
    "\n",
    "ctg_df = data.select_dtypes(include='category') \n",
    "#from select_dtypes documentation (#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)\n",
    "#-> To select strings you must use the object dtype, but note that this will return all object dtype columns\n",
    "num_df = data.select_dtypes(include='float64') \n",
    "\n",
    "#non riconosce i data types perchè li vede tutti direttamente come objects-> andrà sistemato \n",
    "dt = data.select_dtypes(include='object') \n",
    "dt #-> tutti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a1acc",
   "metadata": {},
   "source": [
    "## 2.2 Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990c6b3",
   "metadata": {},
   "source": [
    "The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#il problema è che pandas legge \n",
    "#tutti i valori delle features come stringhe perchè c'è il \"?\" nei missing data            #MISSING DATA\n",
    "#quindi primo step è sostituire il ? con NaN \n",
    "#to then work more seamlessly with the datatypes\n",
    "\n",
    "#guardo dove sta '?'\n",
    "\n",
    "col_list = data.columns\n",
    "data.loc[(data[col_list]=='?').any(axis=1)]\n",
    "#I get that out of the 397 records 240 contains a '?' for one or more features \n",
    "\n",
    "#so the idea is to replace these '?' with NaN so that I can work better with isnull() functions\n",
    "data = data.replace('?', np.NaN)\n",
    "#data.loc[(data[col_list]=='?').any(axis=1)] -> just to check that it actually removed the '?'\n",
    "\n",
    "#now the idea is to deal with missing data \n",
    "#(together with the issue of making pandas read the correct data type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d39bb4",
   "metadata": {},
   "source": [
    "### 2.2.1 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cce66",
   "metadata": {},
   "source": [
    "First get a sense of the missing data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95358ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to manage the missing values\n",
    "#but to handle the missing values I would like to analyze separately \n",
    "#categorical and non categorical features\n",
    "#but I can't do this straightaway because pandas here reads everything\n",
    "#as object since there is '?' for missing values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8fb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so I replace the'?' with nans to analyze the distribution\n",
    "data = data.replace('?', np.NaN)\n",
    "\n",
    "#while processing the rest I discovered there are also \n",
    "# '\\t?' in place of '?' so I fix this as well\n",
    "data = data.replace('\\t?',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the NA and fix the dataframe\n",
    "miss = data.isnull().sum()/len(data)\n",
    "miss = miss[miss>0]\n",
    "miss.sort_values(inplace=True)\n",
    "miss = miss.to_frame()\n",
    "miss.columns = ['count']\n",
    "miss.index.names = ['name']\n",
    "miss['Name'] = miss.index\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf60c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot of the missing \n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.barplot(x = 'Name', y = 'count', data=miss)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Count of missing value per feature')\n",
    "plt.show() \n",
    "#the maximum value of missing values is for rbc (Red Blood Cells) \n",
    "#where 35% is null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap of the missing data \n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(data.isna().transpose(),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "plt.title('Heatmap of missing data')\n",
    "plt.show()\n",
    "#plt.savefig(\"Heatmap_missing_data.png\", dpi=100)\n",
    "\n",
    "#maybe they started to record some of the data (on the left) \n",
    "#after a while \n",
    "#(if we assume data has been collected chronologically from 0 to the last\n",
    "#patient maybe at the beginning they weren't recording characteristics \n",
    "#such as rbc, sod, pot and all those on the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution plot of missing data \n",
    "#plt.figure(figsize=(10,6))\n",
    "sns.displot(\n",
    "    data=data.isna().melt(value_name=\"missing\"),\n",
    "    y=\"variable\",\n",
    "    hue=\"missing\",\n",
    "    multiple=\"fill\", #dodge makes more confusion\n",
    "    aspect=1.25,\n",
    "    palette=[\"C1\", \"C2\"]\n",
    ")\n",
    "plt.title('Distribution plot of missing data')\n",
    "plt.show()\n",
    "#plt.savefig(\"displot_missing.png\", dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c16a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANAYSIS MISSING VALUES FOR CLASS (DISEASE vs NOT DISEASE)\n",
    "#to understand if there is any difference in the presence of missing\n",
    "#values depending on the class\n",
    "\n",
    "#barplot of the missing for different classes \n",
    "\n",
    "#divide the dataset\n",
    "ckd = data[data['class']=='ckd']\n",
    "notckd = data[data['class']=='notckd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14714c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataframes with the two informations \n",
    "\n",
    "nckd = notckd.isnull().sum()/len(notckd)  #missing for those without kidney disease\n",
    "nckd = nckd.to_frame()\n",
    "nckd.columns = ['not_ckd']\n",
    "\n",
    "miss_per_dis = ckd.isnull().sum()/len(ckd) #missing for those with kidney disease\n",
    "miss_per_dis = miss_per_dis.to_frame()\n",
    "miss_per_dis.columns = ['ckd']\n",
    "\n",
    "miss_per_dis['not_ckd'] = nckd['not_ckd'] #merge the two (no sorting so same rows)\n",
    "\n",
    "miss_per_dis['variable'] = miss_per_dis.index #needed for the seaborn plot\n",
    "\n",
    "miss_per_dis #dataframe with the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to represent it with seaborn need to melt down columns so that they can be treated as hue in plotting\n",
    "melted = miss_per_dis.melt('variable', var_name='outcome',value_name='missing')\n",
    "melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x = 'variable', y='missing', hue = 'outcome',data=melted, kind='bar', palette = 'tab10',aspect=2.5)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Count of missing value differentiated per presence/absence of the disease')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a97b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we see  \n",
    "\n",
    "#overall generally fewer missing values for those without the chronic disease\n",
    "#but in the absence of chronic disease the number of missing values is fewer but more spreaded \n",
    "#meaning that all the features have some missing values \n",
    "\n",
    "#whereas for those with the kidney disease \n",
    "#not all features have missing values \n",
    "#(for all patients affected with chronic kidney disease anemia,appet, cas, pe,etc have been record)- blue on the right\n",
    "#but when that feature has missing values the number is generally higher \n",
    "\n",
    "#this tells us that features such as red blood cells count (rbc),rbcc, wbcc,sod,pot,pc \n",
    "#are not very significant to detect chronic kidney disease\n",
    "#and particular attention should be made when dealing with such missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73134510",
   "metadata": {},
   "source": [
    "### 2.2.2 Handling of missing values ( & attribution of correct datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quindi fatta un'idea dei NaN \n",
    "# capisco come gestirli \n",
    "\n",
    "# https://github.com/matthewbrems/ODSC-missing-data-may-18/blob/master/Analysis%20with%20Missing%20Data.pdf\n",
    "# https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n",
    "\n",
    "# prima avevo '?' poi NaN quindi non riesco a dividere categoriche e non \n",
    "# quindi posso adottare un approccio generale e intanto valuto come si distribuiscono i NaN tra le variabili \n",
    "# depending on the number of features for which it has the '?'\n",
    "# if more than 12 (half of the available features) features are '?' then we can reasonably drop the record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f57096",
   "metadata": {},
   "source": [
    "#### Deleting the features for which more than half of the records have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469de31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no features for which more than half of the records have a missing value \n",
    "#(as seen before maximum is 35%)\n",
    "#so we can keep all features -> good, don't suffer of loss of information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f911e",
   "metadata": {},
   "source": [
    "#### Deleting the records that do not have many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297630ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total NaN at each row in a DataFrame and identify the records where this is greater than 10 \n",
    "# reasonable since it's 24 features \n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(data.index)):\n",
    "    rows.append([i+1, data.iloc[i].isnull().sum()])\n",
    "\n",
    "mpr = pd.DataFrame(rows, columns = ['row', 'missing']) #mpr=missing per row\n",
    "many_missing = mpr[mpr['missing']>=10]\n",
    "\n",
    "many_missing #records I want to drop because too many missing features \n",
    "             #-> reasonable to do since the percentage of missing values is not excessive \n",
    "             #in comparison with the complete dataset\n",
    "             #(it's only 11 records (out of 397))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = [many_missing.index[i] for i in range(len(many_missing))] #list of the dataframe\n",
    "\n",
    "data.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "#data.shape -> to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab75bf",
   "metadata": {},
   "source": [
    "#### Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f1a6e",
   "metadata": {},
   "source": [
    "There is no rule of thumb to handle missing data that guarantees a robust model with good performance. \n",
    "There are several sophisticated techniques for the imputation of missing values to prevent the loss informaton while at the same time not bias the analysis and hence create a robust model (eg. imputation using Deep Learning Library Datawig or the prediction of missing values with scikitlearn which takes into account also the covariance between the missing value column and other columns). <br>\n",
    "Since the purpose of this work is more on the visualization of the confusion matrix (than the processing), here the imputation is performed in a more straightforward way. \n",
    "In particular, missing values are substituted with: \n",
    "- the **most frequent** value in the case of *categorical* variables \n",
    "- the **mean** or the **median** depending on the distribution (ie. skewed -> median, non skewed -> mean) in the case of *numerical* variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d7123",
   "metadata": {},
   "source": [
    "##### a) Numerical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can't plot the distribution rightaway because of the issue\n",
    "#of reading everything as object \n",
    "#(since there were '?' then substituted with NANs)\n",
    "#so I make a copy of the dataset without missing to see the distributions\n",
    "#and decide with which value I should substitute the missing \n",
    "#(i.e. median or mean)\n",
    "\n",
    "\n",
    "#copy the dataset\n",
    "dt = data.copy()\n",
    "\n",
    "#select only numerical variables \n",
    "num_feat = ['age','bp','bgr','bu','sc','sod','pot','hemo','pcv','wbcc','rbcc']\n",
    "dt = dt[num_feat]\n",
    "\n",
    "#force the numerical type \n",
    "\n",
    "for f in dt.columns: \n",
    "    dt[f] = dt[f].astype('float')   #non posso farlo con il ciclo for perchè ci sono \\t e errori sparsi\n",
    "                                    #quindi processo feature per feature <- FIXED (c'erano '\\t?' ma poi ho sistemato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots \n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Analysis of the boxplots - broken down per feature \\n', fontsize=22 )\n",
    "\n",
    "red_circle = dict(markerfacecolor='red',marker='o')\n",
    "\n",
    "for col in dt.columns:\n",
    "    sns.boxplot(x=dt[col], flierprops=red_circle) #flierprops per il layout degli outlier\n",
    "    sns.despine()\n",
    "    plt.title(\"Distribution of \"+ col.upper() +'\\n')\n",
    "    plt.tight_layout()                                                      \n",
    "    #plt.savefig('Distribution_{}.png'.format(col), format=\"PNG\") #to save each graph in the directory\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696fc7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several outliers but all reasonable \n",
    "#meaning that there are no clear errors  \n",
    "#(no weird things such as negatives blood pressures or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features (except those below) are relatively skewed so the missing values get substituted with the median\n",
    "# AGE, RBCC, PCV and HEMO are less skewed so the missing values get substituted with the mean\n",
    "ft_mean= ['age','rbcc','pcv','hemo']\n",
    "mean_sub = dt[ft_mean]\n",
    "mean_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f212e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_median = [el for el in list(dt.columns) if el not in ft_mean]\n",
    "median_sub = dt[ft_median]\n",
    "median_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e089360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in median_sub.columns:\n",
    "    median_sub[col].fillna(median_sub[col].median(), inplace=True)\n",
    "    \n",
    "median_sub \n",
    "\n",
    "#just warnings, no need to panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b87ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mean_sub.columns:\n",
    "    mean_sub[col].fillna(mean_sub[col].mean(), inplace=True)\n",
    "    \n",
    "mean_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = pd.concat([mean_sub, median_sub], axis=1)\n",
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fcd8a",
   "metadata": {},
   "source": [
    "##### b) Categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only categorical features \n",
    "#(Note: remember that I am doing this whole thing because it reads \n",
    "#everything as object since there are Nan and '?' \n",
    "#and cannot set it in the read)\n",
    "\n",
    "cat_feat = list(set(data.columns) - set(num_feat))\n",
    "categorical = data[cat_feat]\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute with most frequent value \n",
    "categorical = categorical.fillna(categorical.mode().iloc[0]) \n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while doing the encoding of categorical variables some '\\t' \n",
    "#have emerged so I fix them here \n",
    "\n",
    "print(\"The dm feature has the following categorical values\", categorical['dm'].unique())\n",
    "\n",
    "categorical['dm'].replace({'\\tno':'no','\\tyes':'yes'},inplace= True)\n",
    "\n",
    "\n",
    "print(\"The dm feature has the following categorical values\", categorical['dm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and I separate the target variable from the categorical predictors \n",
    "target = 'class'\n",
    "Y = categorical[target]\n",
    "categorical.drop('class', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740aa505",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c89d8",
   "metadata": {},
   "source": [
    "## 2.3 Exploration of the cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1452d",
   "metadata": {},
   "source": [
    "### Grouped bar chart - categorical features vs target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_chart_data = categorical.copy()\n",
    "bar_chart_data['class'] = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98915afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "cfeatures = list(categorical.columns) #not the class\n",
    "\n",
    "for i in range(len(cfeatures)):\n",
    "    column= cfeatures[i]\n",
    "    sub = fig.add_subplot(3,5,i+1)\n",
    "    chart = sns.countplot(data=bar_chart_data, x=column, hue='class', palette = 'tab10') #alt: RdYlBu, ma così è più unifrome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c597e3",
   "metadata": {},
   "source": [
    "Grouped bar chart is a straightforward representation to show how each categorical value weigh in determining the class. For example, patients with and without hypertension have distinctly distribution of target value, which indicates it is likely to contribute more to the prediction of the target. Similarly,there is a different distribution of the disease according to the presence or absence of Coronary Artery Disease (cad), in the dataset there are no patients with the coronary disease that do not have the chronic kidney disease. \n",
    "<br>\n",
    "Overall, all the displayed categorical features appears to have different distribution of the class according to the categorical value so I expect all features to be relevant in predicting the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebbb23",
   "metadata": {},
   "source": [
    "### Boxplot - numerical features vs target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt_data = numerical.copy()\n",
    "bpt_data['class'] = data['class']\n",
    "bpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeatures = list(numerical.columns)\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "\n",
    "for i in range(len(nfeatures)):\n",
    "    column = nfeatures[i]\n",
    "    sub= fig.add_subplot(3,4, i +1)\n",
    "    sns.boxplot(x='class', y=column, data=bpt_data, palette='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb56924",
   "metadata": {},
   "source": [
    "The boxplot shows how the values of the numerical features vary depending on the class (presence or absence of the kidney disease). For example, Blood Glucose Random (bgr) has a clear difference when the patient is affected by the chronic kidney disease (for those without the disease the blood glucose random tends to be around the same value, whereas for those with the disease there is a greater range of values) so I expect this features to play a greater role in the prediction of the class. This is true also for other features such as age, rbcc and all those with a different distribution. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930eba3",
   "metadata": {},
   "source": [
    "## 2.4 Feature engineering (normalization of numerical predictors, encoding of categorical predictors) and merging of the final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc304c98",
   "metadata": {},
   "source": [
    "### 2.4.1. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the features have different range so I proceed with\n",
    "#Normalization \n",
    "#(=rescaling by the minimum and range of the vector \n",
    "#to make all the elements lie between 0 and 1 \n",
    "#bringing all data to a common scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d441172",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "\n",
    "for col in numerical.columns:\n",
    "    sns.displot(x=numerical[col], binwidth=3, bins=10) \n",
    "    sns.despine()\n",
    "    plt.title(\"Distribution of \"+ col.upper() +'\\n')\n",
    "    plt.tight_layout()                                                      \n",
    "    #plt.savefig('Distribution_{}.png'.format(col), format=\"PNG\") #to save each graph in the directory\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#varying scale and distribution is not gaussian so we normalize \n",
    "#(useful if the algorithm does not make assumptions about the distribution of the data\n",
    "# such as KNN and artifical neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\")\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a530598",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.iloc[:,0:-1] = numerical.iloc[:,0:-1].apply(lambda x: (x-x.mean())/ x.std(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc94c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After:\")\n",
    "numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ba32e",
   "metadata": {},
   "source": [
    "### 2.4.2 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d386562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to convert each text category to numbers in order for the machine \n",
    "#to process them using mathematical equations.\n",
    "\n",
    "#one hot encoding (= each category is represented by a binary vector)\n",
    "#I use one hot encoding so to avoid the situation where I might end up confusing the model into thinking\n",
    "#that a column has data with some kind of order or hierarchy when there is not \n",
    "#since the categorical data is nominal (even specific gravity and albumin)\n",
    "#\n",
    "#(non label encoding perchè con label encoding le categorie sono ranked alphabetically \n",
    "#quindi una categoria risulta migliore di un'altra\n",
    "#e soprattutto con linear models and SVMs we could have problems)\n",
    "\n",
    "#There may be problems when there is no ordinal relationship and allowing the representation \n",
    "#to lean on any such relationship might be damaging to learning to solve the problem.\n",
    "\n",
    "#onehotencoding mi aumenterà molto le colonne ma rimane comunque meno del numero di records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68848f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con SKLEARN\n",
    "\n",
    "#OneHotEncoder() vs pandas.get_dummies()\n",
    "#https://albertum.medium.com/preprocessing-onehotencoder-vs-pandas-get-dummies-3de1f3d77dcc\n",
    "#-> OHE does the same things as get dummies but in addition, OHE saves the exploded categories into it’s object\n",
    "#Saving exploded categories is extremely useful when I want to apply the same data pre-processing on my test set\n",
    "#in quest'applicazione mi fa uguale quindi posso usare get_dummies()\n",
    "#OneHotEncoder Features: \n",
    "\n",
    "#- encode categorical integer features using a one-hot aka one-of-K scheme\n",
    "#- the input to this transformer should be a matrix of integers, denoting the values \n",
    "#  taken on by categorical (discrete) features\n",
    "# -the output will be a sparse matrix where each column corresponds to one possible value of one feature\n",
    "#- it is assumed that input features take on values in the range [0, n_values)\n",
    "#- this encoding is needed for feeding categorical data to many scikit-learn estimators, \n",
    "#  notably linear models and SVMs with the standard kernels\n",
    "\n",
    "#from sklearn.preprocessing import OneHotEncoder \n",
    "#ohe = OneHotEncoder()\n",
    "#X = ohe.fit_transform(categorical).toarray()\n",
    "#X\n",
    "#pd.DataFrame(X)\n",
    "\n",
    "#-> qui fatto con sklearn ma uso get_dummies() che by default fa one-hot encoding\n",
    "#   e comunque non avrò categorie che appaiono nel test set e non nel train quindi dovrebbe andare bene  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ca2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = pd.get_dummies(categorical)\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical.shape \n",
    "#there are more columns due to the encoding\n",
    "#but I still do not encounter issues of high dimensionality \n",
    "#(39 colonne vs 386 records so no issues in the transposing \n",
    "#of the matrix because I still have more records than columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec16d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FEATURE SELECTION \n",
    "#using sklearn \n",
    "\n",
    "#I have a manageable number of features so no need to remove them \n",
    "#the only issue could be with multicollinearity\n",
    "#multicollinearity but should not be too much of an issue in ML predictions\n",
    "#https://towardsdatascience.com/everything-you-need-to-know-about-multicollinearity-2f21f082d6dc\n",
    "#https://stats.stackexchange.com/questions/168622/why-is-multicollinearity-not-checked-in-modern-statistics-machine-learning/168631#168631\n",
    "\n",
    "#however just to be sure I check also the VIF \n",
    "#(measures the ratio between the variance for a given regression coefficient\n",
    "#with only that variable int he model versus the variance for a given \n",
    "#regression coefficient with all variables in the model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86537efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55331cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = numerical.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(numerical.values, i) for i in range(len(numerical.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data\n",
    "#VIF =1 (minimum possible) means the tested predictor is not correlated with the others\n",
    "#the higher the VIF \n",
    "#The more correlated a predictor is with the other predictors\n",
    "#- the more the standard error is inflated\n",
    "#- the larger the confidence interval\n",
    "#- the less likely it is that a coefficient will be evaluated as statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so even if multicollinearity was an issue\n",
    "#this is not the case "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8107ca5",
   "metadata": {},
   "source": [
    "### 2.4.3. Merging of the final dataset and balance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical e numerical features \n",
    "finalDT = pd.concat([numerical, categorical], axis=1)\n",
    "finalDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0934b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable\n",
    "Y\n",
    "Y.replace({\"ckd\": 1, \"notckd\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16360eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final dataset\n",
    "finalDT = pd.concat([finalDT,Y], axis=1)\n",
    "finalDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3189355",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "ax = sns.countplot(x=finalDT[\"class\"])  #palette=['chartreuse', 'darkviolet']\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette('colorblind')\n",
    "plt.title(\"Countplot of the target variable \\n\", fontsize=15, fontweight='bold') \n",
    "ax.set_xticklabels(['Absence of disease','Presence of disease'], fontsize=15)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "#add percentage\n",
    "total = float(len(finalDT))\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.2f}%'.format(100 * p.get_height()/total)\n",
    "    x = p.get_x() + p.get_width()/2 \n",
    "    y = p.get_height()+5\n",
    "    ax.annotate(percentage,(x, y), ha='center', va='center', size=15)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "#unbalanced towards the presence of the chronic kidney disease\n",
    "#generally in medical datasets it's the opposite\n",
    "#but still what I care about is that it's unbalanced\n",
    "#(not really towards which class)\n",
    "#to show the greater robustness of the metric and the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ba9e4",
   "metadata": {},
   "source": [
    "## 2.5 Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d32df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0be972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable\n",
    "y = finalDT['class']\n",
    "y\n",
    "\n",
    "#predictors\n",
    "x = finalDT.copy()\n",
    "x.drop('class', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "#testsize not too big  because I have relatively few records\n",
    "#randomstate for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eee5d5",
   "metadata": {},
   "source": [
    "# 3. Classification and performance assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per ricordarmi di spostare qui tutto quello che ho importato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b91472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ade260",
   "metadata": {},
   "source": [
    "## 3.1 Linear methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba716a",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba95102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learns the probability of a sample belonging to a certain class \n",
    "\n",
    "# discriminative model \n",
    "#(=directly models the posterior probability of P(y|x) y learning the input to output mapping by minimising error)\n",
    "#                      #posterior=update of prob of event A happening given new info as event B happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abea9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "start = timeit.default_timer() #for all models I record the time\n",
    "\n",
    "#instance of the model \n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#the model learns the relationship between predictors and label \n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#predict the label on test set \n",
    "predictionsLR = logreg.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\n",
    "predictionsLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then evaluate\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f910f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmLR = confusion_matrix(y_test,predictionsLR)\n",
    "cmLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINISCI FUNZIONE PER QUESTE METRICHE invece che definirle una per una\n",
    "\n",
    "\"\"\"\n",
    "def matrix_metrix(real_values,pred_values):\n",
    "    CM = confusion_matrix(real_values,pred_values) #get confusion matrix\n",
    "    \n",
    "    TN = CM[0][0]     #confusion matrix entries and n° of samples\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    tot = TN+FN+TP+FP\n",
    "    \n",
    "    #performance metrics with 2 matrix entries\n",
    "    Prevalence = round( (TP+FP) /tot,2)\n",
    "    Accuracy   = round( (TP+TN) / tot,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 ) \n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    \n",
    "    #performance metrics with more than 2 entries -> more comprehensive metrics\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 ) #false positive rate\n",
    "    FNR        = round( FN / (TP+FN),4 ) #false negative rate\n",
    "    TNR        = round( TN / (TN+FP),4 ) #true negative rate \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    \n",
    "    LRPos      = round( Recall/FPR, 4 )   #positive likelihood\n",
    "    LRNeg      = round( FNR / TNR ,4 )   #negative likelihood \n",
    "    \n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "    #FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "    mat_met = pd.DataFrame({\n",
    "        'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','MCC','BM','MK'], #,'FBeta'    \n",
    "        'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,MCC,BM,MK]}) #FBeta\n",
    "    return (mat_met)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOR = (TP / (TP+FN)/FP / (TN+FP))/(FN / (TP+FN)/TN / (TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc572ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_metrix(y_test, predictionsLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "accuracyLR = logreg.score(X_test, y_test)\n",
    "print(accuracyLR)\n",
    "\n",
    "\n",
    "#COMPREHENSIVE METRICS\n",
    "\n",
    "#MCC/phi coefficient\n",
    "#essentially a correlation coefficient between -1(inverse prediction) and 1 (with being 0 average random prediction)\n",
    "#takes into account true and false positives and negatives \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\n",
    "MCCLR = matthews_corrcoef(y_test, predictionsLR)\n",
    "print(MCCLR)\n",
    "\n",
    "\n",
    "#DOR \n",
    "#=TP/FN*FP/TN\n",
    "#DOR = (TP / (TP+FN)/FP / (TN+FP))/(FN / (TP+FN)/TN / (TN+FP))\n",
    "TPLR = cmLR[0][0]\n",
    "FNLR = cmLR[0][1]\n",
    "FPLR = cmLR[1][0]\n",
    "TNLR = cmLR[1][1]\n",
    "\n",
    "DOR = TPLR/FNLR*FPLR/TNLR\n",
    "print(DOR)\n",
    "\n",
    "\n",
    "\n",
    "#F1_score = harmonic mean of precision and recall (0=worst, 1=best)\n",
    "#F1=2*(precision*recall)/(precision+recall)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1LR = f1_score(y_test, predictionsLR, \n",
    "                average=None) #only required for multiclass targets\n",
    "print(F1LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "#from sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec051176",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmLR, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Logistic Regression: {0}'.format(accuracyLR)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#molto alto \n",
    "\n",
    "#errore è effettivamente sul test quindi sta performando molto bene \n",
    "#che è un po' sospetto ma vedo prima tutti gli altri\n",
    "\n",
    "#assumptions della logreg: \n",
    "#- independence of errors, \n",
    "#- linearity in the logit for continuous variables, \n",
    "#- absence of multicollinearity\n",
    "#- lack of strongly influential outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582769d3",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features\n",
    "\n",
    "#1. calculates prior probability for a given class label \n",
    "#2. calculate conditional probability with each attribute for each class\n",
    "#3. multiply same class conditional probability\n",
    "#4. multiply prior probability with step 3 probability\n",
    "#5. sees which class has higher probability, higher probability class belongs to given input set step\n",
    "\n",
    "# generative model\n",
    "# (= models the joint distribution of the feature X and the targetY, \n",
    "#    and then predicts the posterior probability given as P(y|x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#make an instance of the model \n",
    "NB = GaussianNB()\n",
    "\n",
    "#make the model learn the relationship between predictors and label \n",
    "NB.fit(X_train, y_train)\n",
    "\n",
    "#predict the label on test set \n",
    "predictionsNB = NB.predict(X_test)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\n",
    "predictionsNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d07c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmNB = confusion_matrix(y_test,predictionsNB)\n",
    "cmNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyNB = NB.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmNB, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Naive Bayes: {0}'.format(accuracyNB)\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70356b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#anche questo molto alto \n",
    "#ma probabilmente è solo che i modelli lineari si prestano particolarmente a questo dataset \n",
    "#che è piccolo, non high dimensional e abbastanza omogeneo (anche per come sono ho gestito i missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457641c7",
   "metadata": {},
   "source": [
    "### 3.2 Non linear: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does not make any assumption on the data distribution (non parametric)\n",
    "\n",
    "#KNN can be summarized as below:\n",
    "#1.Computes the distance between the new data point with every training example.\n",
    "#2.For computing the distance measures such as Euclidean distance, Hamming distance or Manhattan distance will be used.\n",
    "#3.Model picks K entries in the database which are closest to the new data point.\n",
    "#4.Then it does the majority vote i.e the most common class/label among those K entries will be the class of the new data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#different K to check which to choose\n",
    "k_range = range(1,26)\n",
    "scores = {}\n",
    "scores_list = []\n",
    "for k in k_range: \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    predKNN = knn.predict(X_test)\n",
    "    scores[k] = metrics.accuracy_score(y_test,predKNN)\n",
    "    scores_list.append(metrics.accuracy_score(y_test,predKNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(k_range, scores_list)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "\n",
    "#best appears to be at 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1271ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "predKNN_3k = knn.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\n",
    "predKNN_3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ccf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmKNN = metrics.confusion_matrix(y_test,predKNN_3k)\n",
    "cmKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eae521",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyKNN = knn.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmKNN, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for KNN: {0}'.format(accuracyKNN)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#particolarmente basso rispetto agli altri \n",
    "\n",
    "#probabilmente appunto è la questione della linearità perchè pure i tree performano peggio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65662d69",
   "metadata": {},
   "source": [
    "## 3.2 Tree based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfee7f2",
   "metadata": {},
   "source": [
    "Tend to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920161c",
   "metadata": {},
   "source": [
    "### 3.2.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a396a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE\n",
    "\n",
    "#import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#make instance of the model \n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "#fit the classifier\n",
    "dt = dt.fit(X_train, y_train)\n",
    "\n",
    "#predict response\n",
    "predDT = dt.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\n",
    "predDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd236456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmDT = metrics.confusion_matrix(y_test,predDT)\n",
    "cmDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyDT = dt.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmDT, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Decision Tree: {0}'.format(accuracyDT)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#con i tree va peggio - coerente con il peggioramento del KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48357ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Pip install graphviz\n",
    "#!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b361883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "#from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    " \n",
    "dot_data = StringIO()\n",
    "export_graphviz(dt, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = list(X_train.columns),class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('DecisionTree.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e2a3",
   "metadata": {},
   "source": [
    "### 3.2.2 Bagging Decision Tree (Ensemble learning I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking bootstraps from the training data (=bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf01434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7d23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "bg = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                      max_samples = 0.5,  #maximum size: 50% di tutto il dataset per ogni sample\n",
    "                      max_features = 1.0, #maximum of features: con 1 è 100% quindi tutte le 48 features\n",
    "                      n_estimators = 10)  #number of estimators: il numero di decision trees\n",
    "\n",
    "bg.fit(X_train, y_train)\n",
    "\n",
    "predBG = bg.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "\n",
    "predBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24000e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmBG = metrics.confusion_matrix(y_test,predBG)\n",
    "cmBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyBG = dt.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmBG, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Tree with Bagging: {0}'.format(accuracyBG)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#potrei anche risparmiare questi altri modelli di tree dato che già capisco che tree is not the way to go\n",
    "#ma voglio vedere se è così peggiorata perchè è solo un tree o con gli ensemble migliora\n",
    "\n",
    "#perchè i tree hanno la tendenza a overfittare in generale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9addfe8",
   "metadata": {},
   "source": [
    "### 3.2.3 Boosted Decision Tree (Ensemble learning II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd434d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer() \n",
    "\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(min_samples_split=10,\n",
    "                                                max_depth=4),\n",
    "                                               n_estimators=10,\n",
    "                                               learning_rate=0.6)\n",
    "\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "predBS = adb.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "predBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29de427",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmBS = metrics.confusion_matrix(y_test,predBS)\n",
    "cmBS #tutti true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyBS = dt.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmBS, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Tree with Boosting (AdaBoost): {0}'.format(accuracyBS)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ormai è accanimento terapeutico ma a questo punto vedo anche con random fores\n",
    "\n",
    "#se facessi più valutazioni sugli iperparametri forse migliorerebbe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9252c4b",
   "metadata": {},
   "source": [
    "### 3.2.4 Random Forest (Ensemble learning III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb432e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=30, max_depth=9)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predRF = rf.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "predRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmRF = metrics.confusion_matrix(y_test,predRF)\n",
    "cmRF #tutti true \n",
    "accuracyRF = dt.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmRF, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Tree with Random Forests: {0}'.format(accuracyRF)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8745018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#va beh sembra che con quelli lineari vada meglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc23d6",
   "metadata": {},
   "source": [
    "## 3.3 Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99be966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel (=transforms an input data space into the required form) trick to handle nonlinear input spaces \n",
    "#(to transform the input space to a higher dimensional space so that \n",
    "#then one can easily separate the classes using linear separation)\n",
    "\n",
    "#The classifier separates data points using a hyperplane with the largest amount of margin. \n",
    "#That's why an SVM classifier is also known as a discriminative classifier. \n",
    "#SVM finds a maximum marginal hyperplane (MMH) in multidimensional space which helps in classifying new data points.\n",
    "#(iterative manner that minimizes error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "vect = svm.SVC(kernel='linear')\n",
    "\n",
    "vect.fit(X_train, y_train)\n",
    "\n",
    "predvect = vect.predict(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "predvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmSVM = metrics.confusion_matrix(y_test,predvect)\n",
    "accuracySVM = vect.score(X_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cmKNN, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for SVM: {0}'.format(accuracySVM)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bdabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#questo performa bene ma più tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f255ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "#-kernel\n",
    "#-regularization\n",
    "#–gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#advantages:\n",
    "#–good accuracy and faster predictions wrt NB\n",
    "#-less memory usage because it uses a subset of training points in the decision phase\n",
    "#-works well with high dimensional space (here the features are not too many so the advantage is not extremely \n",
    "# harnessed but still good performance)\n",
    "\n",
    "#disadvantages:\n",
    "#-not suitable for large datasets because of high training time (non è questo il caso)\n",
    "#–sensitive to the type of kernel used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462ae1b",
   "metadata": {},
   "source": [
    "# 4. Visualization of the tetrahedron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c71f6",
   "metadata": {},
   "source": [
    "# Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22c026",
   "metadata": {},
   "source": [
    "Built on top of the Plotly Javascript library (plotly.js), Plotly is an open-source plotting library that enables the creation of interactive web-based visualizations. I use Plotly for three main reasons:  \n",
    "- extreme customization\n",
    "- possibility to display visualizations within Jupyter Notebooks, to save them to standalone html files but also to serve them as part of analytical web-applications using Dash (https://dash.plotly.com/installation) \n",
    "<br>\n",
    "\n",
    "An alternative could have been Ipyvolume which is still open source "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cdbd55",
   "metadata": {},
   "source": [
    "#Python's visualisation landscape\n",
    "#using markdown ![viz](viz_landscape.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#riprendo le cm dai modelli\n",
    "cmLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco una confusion matrix che risulterebbe da una classificazione non auspicabile \n",
    "#per avere l'esempio del punto nel teatredro\n",
    "cm_badc=np.array([[10,30],\n",
    "         [55,5]])\n",
    "cm_badc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e8d16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Mesh3d(\n",
    "        x=[0, 0, 0, 1], #verticalmente ho definito O, C, B, A\n",
    "        y=[0, 0, 1, 0],\n",
    "        z=[0, 1, 0, 0],\n",
    "        \n",
    "        colorbar_title='z',\n",
    "        \n",
    "        colorscale=[[0, 'gold'],\n",
    "                    [0.5, 'mediumturquoise'],\n",
    "                    [1, 'magenta']],\n",
    "        #colors = colorRamp(c(\"red\",'yellow','white','green','blue')),\n",
    "        \n",
    "        # Intensity of each vertex, which will be interpolated and color-coded\n",
    "        intensity=[0, 0.33, 0.66, 1],\n",
    "        \n",
    "        opacity = 0.5, #for transparency\n",
    "        \n",
    "        # i, j and k sono i vertici dei triangoli\n",
    "        # here we represent the 4 triangles of the tetrahedron surface\n",
    "        i=[0, 0, 0, 1],\n",
    "        j=[1, 2, 3, 2],\n",
    "        k=[2, 3, 1, 3],\n",
    "        name='y',\n",
    "        showscale=True\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "#PRIMA QUESTIONE \n",
    "#aggiungere il punto per la confusion matrix dello use case specifico \n",
    "#-> risolto, non metto tutti i modelli ma tipo i due migliori e uno meno performante (da confusion matrix esempio)\n",
    "\n",
    "#Modello LogReg \n",
    "N = sum(sum(cmLR))                  #confusion matrix entries divided by N\n",
    "xLR = [cmLR[0][0]/N] #TP/N\n",
    "yLR = [cmLR[1][1]/N] #TN/N\n",
    "zLR = [cmLR[0][1]/N] #FP/N\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xLR, \n",
    "                           y= yLR, \n",
    "                           z= zLR, \n",
    "                           marker = dict(color='black', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "#Modello DT\n",
    "xDT = [cmDT[0][0]/N] #TP/N\n",
    "yDT = [cmDT[1][1]/N] #TN/N\n",
    "zDT = [cmDT[0][1]/N] #FP/N\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xDT, \n",
    "                           y= yDT, \n",
    "                           z= zDT, \n",
    "                           marker = dict(color='red', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "#metto i punti dei vari modelli così se riesco a mettere il gradiente dovrebbe risultare \n",
    "#facile valutare l'algoritmo migliore \n",
    "\n",
    "\n",
    "#se poi voglio mettere tutti gli altri modelli li aggiungo qui \n",
    "#Modello NB\n",
    "\n",
    "#Modello KNN\n",
    "\n",
    "#Modello SVM \n",
    "\n",
    "\n",
    "\n",
    "#Per avere l'esempio di una confusion matrix \n",
    "#sono tutte relativamente buone confusion matrices \n",
    "#quindi faccio esempio di confusion matrix che uscirebbe da una classificazione non accurata \n",
    "#per far vedere la differenza \n",
    "xB = [cm_badc[0][0]/N] #TP/N\n",
    "yB = [cm_badc[1][1]/N] #TN/N\n",
    "zB = [cm_badc[0][1]/N] #FP/N\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xB, \n",
    "                           y= yB, \n",
    "                           z= zB, \n",
    "                           marker = dict(color='green', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SECONDA QUESTIONE\n",
    "#devo capire come distinguere le due sfumature per avere ABC e AOB verdi/blu\n",
    "#                                                        AOC e BOC arancione/gialli\n",
    "#le sfumature tral'altro lungo OC e lungo AB \n",
    "#-> gradiente come funzione lineare dell'indicatore\n",
    "\n",
    "\n",
    "#devo generare ogni possibile confusion matrix per ogni combinazione\n",
    "#di TP,TN,FP (diviso n)\n",
    "#calcolare mcc associato a ciascuna di questa matrice \n",
    "#rendere il colore \n",
    "\n",
    "#potrei dividere le due mesh (verde/blu e giallo/rosso)\n",
    "\n",
    "#verosimilmente discretizzo (1M points diceva) e per ogni punto calcolo MCC\n",
    "#http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "\n",
    "#piano passante per 3 punti\n",
    "#ax+by+cz+d=0\n",
    "#x+y+z-1=0 equazione del piano nostro http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "#-> la seconda questione poi la risolvo sotto (con meno punti di 1M perchè ci mette la vita altrimenti)\n",
    "\n",
    "#TODO: unire punto di performance dei modelli a tetraedro con tutte le confusion matrices sotto\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb441e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUI GENERO I DATI PER IL GRADIENTE DEL TETRAEDRO \n",
    "\n",
    "#devo capire come distinguere le due sfumature per avere ABC e AOB verdi/blu\n",
    "#                                                        AOC e BOC arancione/gialli\n",
    "#le sfumature tral'altro lungo OC e lungo AB \n",
    "#-> gradiente come funzione lineare dell'indicatore\n",
    "\n",
    "\n",
    "#devo generare ogni possibile confusion matrix per ogni combinazione\n",
    "#di TP,TN,FP (diviso n)\n",
    "#calcolare mcc associato a ciascuna di questa matrice \n",
    "#rendere il colore \n",
    "\n",
    "#potrei dividere le due mesh (verde/blu e giallo/rosso)\n",
    "\n",
    "#verosimilmente discretizzo (1M points diceva) e per ogni punto calcolo MCC\n",
    "#http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "\n",
    "#piano passante per 3 punti\n",
    "#ax+by+cz+d=0\n",
    "#x+y+z-1=0 equazione del piano nostro http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#COMMENTO PER NON AVERCELO OGNI VOLTA CHE ESEGUO LE APPLICAZIONI SOTTO \n",
    "#MA SCOMMENTANDOLO SI HA RISULTATO \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#definisco 3 vettori x,y,z discretizzati da 0 a 1\n",
    "xd = np.linspace(0,1, num=100) #metto intanto pochi punti\n",
    "yd = np.linspace(0,1, num=100)\n",
    "zd = np.linspace(0,1, num=100)\n",
    "\n",
    "#voglio creare un vettore di x,y,z per avere tutti i punti del cubo \n",
    "#lo faccio definendo \n",
    "#x>0 con x= 0->1\n",
    "#y>0     y= 0->1\n",
    "#z>0     z= 0->1\n",
    "\n",
    "#e poi filtrare per quelli che stanno sotto il piano x+y+z-1=0\n",
    "#cioè z < 1 - x - y (seconda condizione da soddisfare)\n",
    "#points = [xd.T,yd.T,zd.T] #per avere matrice di vettori colonna\n",
    "#points\n",
    "\n",
    "#points=np.meshgrid(xd,yd,zd,indexing='ij')\n",
    "#points\n",
    "\n",
    "all_p_array = np.array(np.meshgrid(xd, yd, zd)).T.reshape(-1,3)\n",
    "#len(all_p_array) just to check\n",
    "\n",
    "column_values = ['x','y','z']\n",
    "all_p_df = pd.DataFrame(data=all_p_array,\n",
    "                        columns=column_values)\n",
    "all_p_df\n",
    "#print(len(all_p_df))\n",
    "\n",
    "#adesso voglio droppare le righe se stanno sopra il piano\n",
    "#cioè se z > 1 -x -y\n",
    "filtered = all_p_df.query('z<1-x-y')\n",
    "filtered\n",
    "#ha senso che sia 1/5 perchè ne avrai che si ripetono 4 e uno in centro\n",
    "\n",
    "#temporaneo -brutto codice \n",
    "#filtered['size']= 12\n",
    "#filtered.loc[:, 'size'] = 5\n",
    "\n",
    "\n",
    "#aggiungo colonna MCC\n",
    "\n",
    "#sklearn usa le predizioni non le entries della confusion matrix \n",
    "#quindi lo ridefinisco \n",
    "#MCC = (TP*TN -FP*FN)/sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))\n",
    "#x è TP quindi row.x\n",
    "#y è TN quindi row.y\n",
    "#z è FP quindi row.z\n",
    "#    FN = 1-FP-TP-TN quindi 1-row.z-row.x-row.y\n",
    "\n",
    "#è scaling invariant quindi facendo i conti n va via\n",
    "\n",
    "filtered['MCC'] = filtered.apply(\n",
    "    lambda row: (row.x*row.y - row.z*(1-row.z-row.x-row.y))/math.sqrt((row.x+row.z)*(row.x+(1-row.z-row.x-row.y))*(row.y+row.z)*(row.y +(1-row.z-row.x-row.y))),\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "filtered\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#da sistemare i NAN\n",
    "\n",
    "\"\"\"\n",
    "#----------- Qui per il plot dei modelli\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Mesh3d(\n",
    "        x=[0, 0, 0, 1], #verticalmente ho definito O, C, B, A\n",
    "        y=[0, 0, 1, 0],\n",
    "        z=[0, 1, 0, 0],\n",
    "        \n",
    "        colorbar_title='z',\n",
    "        \n",
    "        colorscale=[[0, 'gold'],\n",
    "                    [0.5, 'mediumturquoise'],\n",
    "                    [1, 'magenta']],\n",
    "        #colors = colorRamp(c(\"red\",'yellow','white','green','blue')),\n",
    "        \n",
    "        # Intensity of each vertex, which will be interpolated and color-coded\n",
    "        intensity=[0, 0.33, 0.66, 1],\n",
    "        \n",
    "        opacity = 0.1, #for transparency\n",
    "        \n",
    "        # i, j and k sono i vertici dei triangoli\n",
    "        # here we represent the 4 triangles of the tetrahedron surface\n",
    "        i=[0, 0, 0, 1],\n",
    "        j=[1, 2, 3, 2],\n",
    "        k=[2, 3, 1, 3],\n",
    "        name='y',\n",
    "        showscale=True\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "#PRIMA QUESTIONE \n",
    "#aggiungere il punto per la confusion matrix dello use case specifico \n",
    "#-> risolto, non metto tutti i modelli ma tipo i due migliori e uno meno performante (da confusion matrix esempio)\n",
    "\n",
    "#Modello LogReg \n",
    "N = sum(sum(cmLR))                  #confusion matrix entries divided by N\n",
    "xLR = [cmLR[0][0]/N] #TP/N\n",
    "yLR = [cmLR[1][1]/N] #TN/N\n",
    "zLR = [cmLR[0][1]/N] #FP/N\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xLR, \n",
    "                           y= yLR, \n",
    "                           z= zLR, \n",
    "                           marker = dict(color='black', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "#Modello DT\n",
    "xDT = [cmDT[0][0]/N] #TP/N\n",
    "yDT = [cmDT[1][1]/N] #TN/N\n",
    "zDT = [cmDT[0][1]/N] #FP/N\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xDT, \n",
    "                           y= yDT, \n",
    "                           z= zDT, \n",
    "                           marker = dict(color='red', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "#metto i punti dei vari modelli così se riesco a mettere il gradiente dovrebbe risultare \n",
    "#facile valutare l'algoritmo migliore \n",
    "\n",
    "\n",
    "#se poi voglio mettere tutti gli altri modelli li aggiungo qui \n",
    "#Modello NB\n",
    "\n",
    "#Modello KNN\n",
    "\n",
    "#Modello SVM \n",
    "\n",
    "\n",
    "\n",
    "#Per avere l'esempio di una confusion matrix \n",
    "#sono tutte relativamente buone confusion matrices \n",
    "#quindi faccio esempio di confusion matrix che uscirebbe da una classificazione non accurata \n",
    "#per far vedere la differenza \n",
    "xB = [cm_badc[0][0]/N] #TP/N\n",
    "yB = [cm_badc[1][1]/N] #TN/N\n",
    "zB = [cm_badc[0][1]/N] #FP/N\n",
    "fig.add_trace(go.Scatter3d(mode='markers', \n",
    "                           x= xB, \n",
    "                           y= yB, \n",
    "                           z= zB, \n",
    "                           marker = dict(color='green', size=5), showlegend=False))\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#---------- Da qui sotto solo per il tetraedro con tutte le confusion matrix e indicatore \n",
    "\n",
    "#SECONDA QUESTIONE\n",
    "#devo capire come distinguere le due sfumature per avere ABC e AOB verdi/blu\n",
    "#                                                        AOC e BOC arancione/gialli\n",
    "#le sfumature tral'altro lungo OC e lungo AB \n",
    "#-> gradiente come funzione lineare dell'indicatore\n",
    "\n",
    "\n",
    "#devo generare ogni possibile confusion matrix per ogni combinazione\n",
    "#di TP,TN,FP (diviso n)\n",
    "#calcolare mcc associato a ciascuna di questa matrice \n",
    "#rendere il colore \n",
    "\n",
    "#potrei dividere le due mesh (verde/blu e giallo/rosso)\n",
    "\n",
    "#verosimilmente discretizzo (1M points diceva) e per ogni punto calcolo MCC\n",
    "#http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "\n",
    "#piano passante per 3 punti\n",
    "#ax+by+cz+d=0\n",
    "#x+y+z-1=0 equazione del piano nostro http://al-roomi.org/3DPlot/index.html\n",
    "\n",
    "#-> la seconda questione poi la risolvo sotto (con meno punti di 1M perchè ci mette la vita altrimenti)\n",
    "\n",
    "#TODO: unire punto di performance dei modelli a tetraedro con tutte le confusion matrices sotto\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(filtered, \n",
    "                    x='x', \n",
    "                    y='y', \n",
    "                    z='z',\n",
    "                    #mode='markers',\n",
    "                    #marker = dict(size=12,\n",
    "                    #              #color=filtered['MCC'],\n",
    "                    #             colorscale='Viridis',\n",
    "                    #             opacity=0.8)\n",
    "                    \n",
    "                    #size = 'size',\n",
    "                    opacity = 0.3,\n",
    "                    color='MCC'\n",
    "                   )\n",
    "\n",
    "fig.update_traces(marker=dict(size=12,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='circle-open'))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753da40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fccdb1",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58f98f",
   "metadata": {},
   "source": [
    "## 5.1 Global landscape for classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#questo una volta che sistemo gradiente e punto dentro al tetraedro sopra è pronto\n",
    "#perchè basta che aggiungo due colonne al dataset, una per DOR e una per F1 e faccio vedere la differenza di colore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c895d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bb454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a3bca5c",
   "metadata": {},
   "source": [
    "## 5.2 Trajectories and ROC in Confusion Tetrahedron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qui faccio vedere differenza con ROC curve - basta fare i plot ma voglio finire prima quella delle epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf517a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328a551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b76e7d0c",
   "metadata": {},
   "source": [
    "## 5.3 Trajectories of Confusion Matrices in training epochs across layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362ba8c",
   "metadata": {},
   "source": [
    "Qui parte sezione di Deep Learning con Neural Network - se ho capito giusto dovrei far vedere che il punto nelle varie epoche si sposta e progressivamente migliora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d53ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow\n",
    "from keras import losses\n",
    "from keras import metrics as metricske"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split an additional validation dataset - I don't have many records so I pick less than half\n",
    "#-> so probably greater tendency to overfit \n",
    "\n",
    "N_nn = 100\n",
    "X_validation=X_train[:N_nn]\n",
    "X_part_train=X_train[N_nn:]\n",
    "\n",
    "y_validation=y_train[:N_nn]\n",
    "y_part_train=y_train[N_nn:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback #callback vengono chiamate quando qualcosa succede\n",
    "#ci sono tante callback (intervengono alla conclusione di ogni iter), io le uso per andare a operare su ogni epoca \n",
    "from keras import backend as K\n",
    "\n",
    "#inizializzo lista in cui ad ogni epoca aggiungerò\n",
    "lst_points = [] #TODO:meglio usare numpy probabilmente per poi trasformare in dataframe\n",
    "\n",
    "                                #creo classe GetEpochOutput che \n",
    "class GetEpochOutput(Callback): #eredita la classe callback di Keras (per poter modificare le callback)       \n",
    "    def __init__(self, x_test, y_test, lst): #costruttore - gli passerò come oggetto X_test e lst_points\n",
    "        self.lst = lst\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #prediction = self.model.predict_class(self.x_test)\n",
    "        prediction = self.model.predict(self.x_test).round()\n",
    "        print(prediction)\n",
    "        cm = confusion_matrix(prediction, self.y_test)\n",
    "        x_epoch = [cm[0][0]/N_nn] #TP/N check that N is actually the size of validation!!\n",
    "        y_epoch = [cm[1][1]/N_nn] #TN/N\n",
    "        z_epoch = [cm[0][1]/N_nn] #FP/N\n",
    "        self.lst.append([x_epoch,y_epoch,z_epoch])  #TODO:usa metodi di numpy!        \n",
    "        #print(self.tetragon)\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#instatiate the model\n",
    "NN=models.Sequential()\n",
    "\n",
    "#input layer\n",
    "NN.add(layers.Dense(48,activation='sigmoid',input_shape=(48,)))\n",
    "\n",
    "#hidden layer 1\n",
    "NN.add(layers.Dense(24,activation='relu'))\n",
    "\n",
    "#hidden layer 2\n",
    "NN.add(layers.Dense(24,activation='relu'))\n",
    "\n",
    "#hidden layer 3\n",
    "NN.add(layers.Dense(24,activation='relu'))\n",
    "\n",
    "#output layer\n",
    "NN.add(layers.Dense(1,activation='hard_sigmoid'))\n",
    "\n",
    "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, nesterov=True)\n",
    "\n",
    "NN.compile(optimizer='sgd',\n",
    "           loss='binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "\n",
    "NN.fit(X_part_train,\n",
    "       y_part_train,\n",
    "       epochs= 200, #epoch= one learning cycle where the learner sees the whole training set\n",
    "       batch_size=50,\n",
    "       validation_data=(X_validation,y_validation),\n",
    "       callbacks = [GetEpochOutput(X_validation, y_validation, lst_points)]\n",
    "      )\n",
    "\n",
    "lst_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65caec39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert list of points into dataframe [\"x_point\",\"y_point\",\"z_point\", \"epoch\"]\n",
    "#TODO\n",
    "points_df = px.data.gapminder()\n",
    "\n",
    "fig = px.scatter_3d(points_df, x=\"x_point\", y=\"y_point\", z=\"z_point\", animation_frame=\"epoch\", animation_group=\"country\",\n",
    "           size=\"pop\", hover_name=\"epoch\") #hover_name posso cambiare\n",
    "\n",
    "fig[\"layout\"].pop(\"updatemenus\") # optional, drop animation buttons\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c0bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdba0ab8",
   "metadata": {},
   "source": [
    "# Different distribution of confmats in stratified vs non stratified resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-balance-a-dataset-in-python-36dff9d12704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211dce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified split -> dovrebbe esserci parametro su sklearn\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=4, stratify=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
